ğŸš€ Curitiba Public Finance ETL

Modern Data Engineering Pipeline with Airflow, Postgres and SQL

Este projeto Ã© um pipeline ETL (Extract, Transform, Load) completo e profissional, projetado para processar dados pÃºblicos de Receita e Despesa da cidade de Curitiba. Ele demonstra orquestraÃ§Ã£o de nÃ­vel de produÃ§Ã£o com Apache Airflow e um Data Warehouse multi-camadas utilizando PostgreSQL e Docker.

A arquitetura segue o modelo clÃ¡ssico de camadas: Staging, Silver e Gold.

ğŸŒŸ Key Highlights

Categoria

Detalhe

BenefÃ­cio para PortfÃ³lio

OrquestraÃ§Ã£o

Pipeline ETL End-to-End com Airflow e TaskGroups.

Demonstra domÃ­nio em agendamento e modularidade.

Modelagem

Data Warehouse Multi-camadas (Staging/Silver/Gold) e Star Schema.

Prova conhecimento em arquitetura e design de dados.

Infraestrutura

Ambiente totalmente containerizado com Docker Compose.

Mostra proficiÃªncia em ferramentas DevOps e reprodutibilidade.

TransformaÃ§Ã£o

SQL-Driven Transformations (DDL/DML) com scripts modulares.

Evidencia habilidades sÃ³lidas em SQL e Engenharia de Dados.

Dados

UtilizaÃ§Ã£o de datasets pÃºblicos e reais de fontes governamentais.

Agrega valor de negÃ³cio e relevÃ¢ncia prÃ¡tica.

ğŸ—ï¸ Arquitetura do Data Pipeline

O fluxo de dados Ã© rigorosamente dividido em camadas para garantir a qualidade, rastreabilidade e desempenho analÃ­tico.

graph LR
    A[CSV Data] --> B(Python Ingestion);
    B --> C(Postgres :: Staging);
    C --> D(SQL Transformations :: Silver);
    D --> E(Dimensional Modeling :: Gold);
    E --> F(Analytics Tools: Power BI, Metabase);


ğŸ§  O que Este Projeto Demonstra

Airflow Orchestration: Uso de TaskGroup para organizaÃ§Ã£o, dependÃªncias claras e reutilizaÃ§Ã£o de cÃ³digo Python/SQL.

Data Modeling: CriaÃ§Ã£o das dimensÃµes (dim_tempo, dim_orgao, dim_fonte) e fatos (fato_receita, fato_despesa).

Pipeline Design: PrincÃ­pios de idempotÃªncia e reprodutibilidade aplicados em todas as camadas.

ğŸ”„ Airflow Pipeline Overview

O DAG (etl_curitiba_financas_dag.py) Ã© estruturado da seguinte forma:

TaskGroup

Responsabilidade

Tarefas SQL Chave

staging

IngestÃ£o inicial dos dados brutos.

create_staging_tables

silver

TransformaÃ§Ãµes de limpeza e padronizaÃ§Ã£o.

build_silver_layer

gold_dimensions

ConstruÃ§Ã£o das tabelas Dimensionais.

dim_tempo, dim_orgao, dim_fonte

gold_facts

ConstruÃ§Ã£o das tabelas de Fato (Star Schema).

fato_receita, fato_despesa

ğŸ› ï¸ Tech Stack

Tecnologia

VersÃ£o

PropÃ³sito

Apache Airflow

Latest (Docker)

OrquestraÃ§Ã£o do Pipeline

PostgreSQL

Latest (Docker)

Data Warehouse e RepositÃ³rio de Metadados

Docker Compose

Latest

DefiniÃ§Ã£o e Gerenciamento da Infraestrutura

Python

3.x

IngestÃ£o (ETL) e Operadores Airflow

SQL (Postgres)

-

TransformaÃ§Ãµes (DML e DDL)

ğŸ“¦ Repository Structure

curitiba-financas-etl/
â”‚
â”œâ”€â”€ dags/
â”‚   â”œâ”€â”€ etl_curitiba_financas_dag.py    # DefiniÃ§Ã£o do DAG principal
â”‚   â””â”€â”€ sql/                           # Scripts SQL modularizados por camada
â”‚       â”œâ”€â”€ create_staging_tables.sql
â”‚       â”œâ”€â”€ build_silver.sql
â”‚       â”œâ”€â”€ dim_tempo.sql
â”‚       â”œâ”€â”€ dim_orgao.sql
â”‚       â”œâ”€â”€ dim_fonte.sql
â”‚       â”œâ”€â”€ fato_receita.sql
â”‚       â””â”€â”€ fato_despesa.sql
â”‚
â”œâ”€â”€ airflow/                           # ConfiguraÃ§Ã£o do ambiente Docker
â”‚   â”œâ”€â”€ docker-compose.yaml            # DefiniÃ§Ã£o dos serviÃ§os
â”‚   â””â”€â”€ Dockerfile                     # Imagem customizada do Airflow (se aplicÃ¡vel)
â”‚
â””â”€â”€ README.md


âš™ï¸ Como Rodar o Projeto (Setup)

PrÃ©-requisitos: Certifique-se de ter o Docker e o Docker Compose instalados.

Clone o RepositÃ³rio:

git clone [https://github.com/seu-usuario/curitiba-financas-etl.git](https://github.com/seu-usuario/curitiba-financas-etl.git)
cd curitiba-financas-etl


Inicie a Infraestrutura:
Execute o Docker Compose para subir os serviÃ§os do Airflow e PostgreSQL:

docker-compose -f airflow/docker-compose.yaml up -d --build


Acesse o Airflow:

URL: http://localhost:8080

Login: Use as credenciais definidas no docker-compose.yaml (geralmente airflow/airflow).

Execute o DAG:

Localize o DAG etl_curitiba_financas_dag na interface.

Ative-o e dispare uma execuÃ§Ã£o manual para iniciar o pipeline ETL completo.
