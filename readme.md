ğŸ“Š Curitiba Public Finance ETL â€“ Data Engineering Pipeline with Airflow

This project implements a complete ETL pipeline using public Revenue and Expense datasets from the city of Curitiba (Brazil).
The entire workflow is orchestrated by Apache Airflow, uses PostgreSQL as a Data Warehouse, and follows a modern multi-layer architecture (Staging â†’ Silver â†’ Gold) aligned with Data Engineering standards.

ğŸš€ Project Purpose

Build a modular, scalable and production-style data pipeline that:

Ingests monthly CSV datasets published by the city

Creates raw staging tables

Cleans and transforms data into the silver layer

Builds analytical dimensional models (star schema)

Generates fact tables for financial analysis

Automates everything using Apache Airflow

This pipeline enables deep insights into public spending and revenue behavior.

ğŸ—ï¸ Architecture Overview
ğŸ“ CSV Data â†’ ğŸ Python Ingestion â†’ ğŸ—„ï¸ Postgres (Staging)
       â†“
   ğŸ› ï¸ SQL Transformations (Silver)
       â†“
 ğŸŒŸ Gold Layer (Dimensions + Facts)
       â†“
ğŸ“Š Analytics (Metabase / PowerBI / Superset)

ğŸ”„ Airflow Pipeline Flow
pipeline_staging
   â””â”€â”€ create_staging_tables
        â†“
pipeline_silver
   â””â”€â”€ build_silver_layer
        â†“
pipeline_gold_dimensions  (runs in parallel)
   â”œâ”€â”€ build_dim_tempo
   â”œâ”€â”€ build_dim_orgao
   â””â”€â”€ build_dim_fonte
        â†“
pipeline_gold_facts  (runs in parallel)
   â”œâ”€â”€ build_fato_receita
   â””â”€â”€ build_fato_despesa


Airflow uses TaskGroup to visually separate logical steps.

ğŸ“¸ Example Airflow UI

(Replace with your own screenshots)

Staging â†’ Silver â†’ Gold Dimensions (parallel) â†’ Gold Facts (parallel)

ğŸ“‚ Project Structure
curitiba-financas-airflow-etl/
â”‚
â”œâ”€â”€ dags/
â”‚   â”œâ”€â”€ etl_curitiba_financas_dag.py     # Airflow DAG with pipelines
â”‚   â””â”€â”€ sql/                             # SQL scripts used by the pipeline
â”‚       â”œâ”€â”€ create_staging_tables.sql
â”‚       â”œâ”€â”€ build_silver.sql
â”‚       â”œâ”€â”€ dim_tempo.sql
â”‚       â”œâ”€â”€ dim_orgao.sql
â”‚       â”œâ”€â”€ dim_fonte.sql
â”‚       â”œâ”€â”€ fato_receita.sql
â”‚       â””â”€â”€ fato_despesa.sql
â”‚
â”œâ”€â”€ src/etl/
â”‚   â”œâ”€â”€ ingest_receitas.py               # optional ingestion scripts
â”‚   â”œâ”€â”€ ingest_despesas.py
â”‚
â”œâ”€â”€ airflow/                             # Docker-based Airflow environment
â”‚   â”œâ”€â”€ docker-compose.yaml
â”‚   â””â”€â”€ Dockerfile (if used)
â”‚
â”œâ”€â”€ sql/                                  # SQL originals (local dev)
â”‚
â””â”€â”€ README.md

ğŸ› ï¸ Technologies Used
Technology	Purpose
Apache Airflow 2.8+	Pipeline orchestration
PostgreSQL 13	Data warehouse (staging, silver, gold layers)
Docker Compose	Environment containerization
Python 3.10+	CSV ingestion and transformations
Pandas	Raw data handling
SQL (PostgreSQL)	Silver and gold layer modeling
TaskGroup	Organized pipeline grouping in Airflow
âš™ï¸ How to Run the Project
1. Clone the Repository
git clone https://github.com/YOUR-USER/curitiba-financas-airflow-etl.git
cd curitiba-financas-airflow-etl

2. Start the Airflow Environment

Inside the Airflow folder:

cd airflow
docker compose up -d


Access the Airflow UI:

http://localhost:8080


Default login:

User: admin

Password: admin

3. Set Up the Airflow Connection

Navigate to:
Admin â†’ Connections â†’ Add Connection

Field	Value
Conn Id	curitiba_postgres
Conn Type	Postgres
Host	postgres
Schema	curitiba_financas
Login	airflow
Password	airflow
Port	5432

Click Test â€” it must show Success.

4. Make SQL scripts available to Airflow
copy .\sql\*.sql .\dags\sql\


Verify inside the container:

docker exec -it airflow-webserver-1 ls /opt/airflow/dags/sql

5. Run the Pipeline

In the Airflow UI:

Enable the DAG etl_curitiba_financas

Click Trigger DAG

Watch the Staging â†’ Silver â†’ Gold pipeline run

ğŸ“š Data Warehouse Modeling
ğŸŸª Silver Layer

Standardized fields and derived attributes:

Year/Month fields

Cleaned and normalized values

Proper typing (numeric/date)

â­ Gold Dimensions

dim_tempo â€” calendar dimension

dim_orgao â€” government organizations

dim_fonte â€” funding source (merged from expenses + revenues)

ğŸ’  Gold Facts
Fact Tables

fato_receita

Aggregated revenue by date, source, company, and type

fato_despesa

Summarized expenses by date, orgÃ£o, program, action, and function

ğŸ“Š Possible Analytics

This DW enables analyses like:

Monthly revenue trends

Expense behavior by government organization

Budget vs actual spending

Analysis by program, action, or function

Comparison between funding sources

Revenue vs payments over time

ğŸ”® Future Improvements (Roadmap)

 Add file sensors for automated ingestion

 Build ingestion directly from Curitiba Transparency Portal

 Add Data Quality checks (SQL or Great Expectations)

 Publish Gold layer in Parquet (Lakehouse)

 Add dashboards (Superset / Metabase / PowerBI)

 Make pipeline incremental by month

ğŸ“ License

MIT License - free for academic, personal, and professional use.
