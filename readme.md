# ğŸš€ Curitiba Public Finance ETL

## Modern Data Engineering Pipeline with Airflow, Postgres and SQL

Este projeto Ã© um *pipeline* ETL (Extract, Transform, Load) completo, projetado para processar dados pÃºblicos de **Receita e Despesa** da prefeitura de **Curitiba**. Ele demonstra orquestraÃ§Ã£o de nÃ­vel de produÃ§Ã£o, transformaÃ§Ãµes em SQL, modelagem de dados e infraestrutura containerizada utilizando **Apache Airflow** e **Docker**.

A arquitetura segue a abordagem clÃ¡ssica de mÃºltiplas camadas de dados: **Staging, Silver e Gold**.

---

## ğŸŒŸ Destaques do Projeto

* **Pipeline ETL Ponta a Ponta** orquestrado com **Airflow**.
* OrquestraÃ§Ã£o do *pipeline* baseada em **TaskGroup** para modularidade.
* **Data Warehouse Multi-camadas:** Staging, Silver e Gold.
* Modelagem **Star Schema** com tabelas de fatos e dimensÃµes.
* TransformaÃ§Ãµes guiadas por **SQL** com arquivos modulares e limpos.
* Ambiente totalmente **containerizado** com **Docker Compose**.
* Utiliza *datasets* pÃºblicos e reais de fontes governamentais.
* **Ideal para entrevistas e demonstraÃ§Ãµes de portfÃ³lio.**

---

## ğŸ—ï¸ Arquitetura do Data Pipeline

O fluxo de dados segue a progressÃ£o atravÃ©s das camadas do Data Warehouse:

```mermaid
graph LR
    A[CSV Data] --> B(Python Ingestion);
    B --> C(Postgres :: Staging);
    C --> D(SQL Transformations :: Silver);
    D --> E(Dimensional Modeling :: Gold);
    E --> F(Analytics Tools: Power BI, Metabase);
O que o projeto demonstra:ConceitoDescriÃ§Ã£oOrquestraÃ§Ã£o AirflowAgendamento, retentativas, dependÃªncias de tarefas, SQL com Jinja, tarefas Python e pipelines modulares.Modelagem de DadosCriaÃ§Ã£o de dimensÃµes e fatos: dim_tempo, dim_orgao, dim_fonte, fato_receita e fato_despesa.SQL EngineeringScripts DDL (Data Definition Language) e DML (Data Manipulation Language) claros para cada etapa de transformaÃ§Ã£o.Design de PipelineSeparaÃ§Ã£o de camadas, idempotÃªncia, reprodutibilidade e engenharia modular.ğŸ”„ VisÃ£o Geral do Pipeline no AirflowO Directed Acyclic Graph (DAG) Ã© estruturado em TaskGroups para representar as camadas do Data Warehouse:TaskGroupTarefas (Tarefas SQL)Objetivostagingcreate_staging_tablesIngestÃ£o inicial dos dados brutos para o PostgreSQL.silverbuild_silver_layerTransformaÃ§Ãµes bÃ¡sicas e limpeza dos dados.gold_dimensionsdim_tempo, dim_orgao, dim_fonteConstruÃ§Ã£o das tabelas de DimensÃ£o.gold_factsfato_receita, fato_despesaConstruÃ§Ã£o das tabelas de Fato (Star Schema).ğŸ› ï¸ Tech StackTecnologiaFinalidadeApache AirflowOrquestraÃ§Ã£o e Agendamento do Pipeline.PostgreSQLData Warehouse (Staging, Silver, Gold).Docker ComposeGerenciamento e ConfiguraÃ§Ã£o da Infraestrutura.PythonIngestÃ£o de dados CSV e Operadores Airflow.SQL (Postgres)LÃ³gica de TransformaÃ§Ã£o (T e L no ETL).ğŸ“¦ Estrutura do RepositÃ³riocuritiba-financas-etl/
â”‚
â”œâ”€â”€ dags/
â”‚   â”œâ”€â”€ etl_curitiba_financas_dag.py    # DefiniÃ§Ã£o do DAG principal do Airflow
â”‚   â””â”€â”€ sql/                           # Scripts SQL para transformaÃ§Ãµes
â”‚       â”œâ”€â”€ create_staging_tables.sql
â”‚       â”œâ”€â”€ build_silver.sql
â”‚       â”œâ”€â”€ dim_tempo.sql
â”‚       â”œâ”€â”€ dim_orgao.sql
â”‚       â”œâ”€â”€ dim_fonte.sql
â”‚       â”œâ”€â”€ fato_receita.sql
â”‚       â””â”€â”€ fato_despesa.sql
â”‚
â”œâ”€â”€ airflow/                           # Arquivos para a construÃ§Ã£o do ambiente Airflow/Postgres
â”‚   â”œâ”€â”€ docker-compose.yaml            # DefiniÃ§Ã£o dos serviÃ§os (Airflow, Postgres)
â”‚   â””â”€â”€ Dockerfile                     # Imagem customizada do Airflow (se aplicÃ¡vel)
â”‚
â””â”€â”€ README.md
ğŸ“Š Possibilidades AnalÃ­ticasO modelo dimensional final (Camada Gold) permite anÃ¡lises robustas como:Monitoramento da tendÃªncia de Receita.AnÃ¡lise do comportamento de Despesas por Ã“rgÃ£o.ComparaÃ§Ãµes entre OrÃ§amento planejado e valores executados.AnÃ¡lise das principais Fontes de Financiamento.Comportamento de sÃ©ries temporais mÃªs a mÃªs.ğŸ¯ Por Que Este Projeto se DestacaUtiliza um dataset pÃºblico e real com significado de negÃ³cio.Replica uma arquitetura de Data Warehouse profissional de mÃºltiplas camadas.Demonstra domÃ­nio de Airflow, SQL e containerizaÃ§Ã£o.CÃ³digo limpo e modular seguindo as melhores prÃ¡ticas de Engenharia de Dados.Ambiente pronto para rodar, reproduzir e apresentar em entrevistas.âš™ï¸ Como Rodar o ProjetoPrÃ©-requisitos: Certifique-se de ter o Docker e o Docker Compose instalados.Clone o RepositÃ³rio:Bashgit clone [https://github.com/seu-usuario/curitiba-financas-etl.git](https://github.com/seu-usuario/curitiba-financas-etl.git)
cd curitiba-financas-etl
Suba a Infraestrutura (via Docker Compose):Bashdocker-compose -f airflow/docker-compose.yaml up -d
Isso iniciarÃ¡ os serviÃ§os do Airflow e PostgreSQL.Acesse o Airflow:Acesse http://localhost:8080 (porta padrÃ£o do Airflow).FaÃ§a login (usuÃ¡rio e senha geralmente configurados no docker-compose.yaml, ex: airflow/airflow).Execute o DAG:Localize o DAG etl_curitiba_financas_dag.Ative-o e execute-o manualmente. O pipeline irÃ¡ ingerir os dados e rodar todas as transformaÃ§Ãµes SQL.Verifique os Dados:Conecte-se ao serviÃ§o PostgreSQL para inspecionar as tabelas nas camadas Staging, Silver e Gold.
